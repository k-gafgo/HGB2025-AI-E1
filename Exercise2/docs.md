# Activity 1
*Considering the above part ```Debezium CDC with PostgreSQL and Kafka```, explain with your own words what it does and why it is a relevant software architecture for Big Data in the AI era and for which use cases.*

Debezium CDC watches the PostreSQL-DB for changes (insert, update, delete) and automatically produces Kafka messages informing about the modifications that have been done. 

One advantage is that multiple systems can independently consume the data, without touching the DB at all. 

AI systems often need real-time data, which CDC makes possible here.

Use-case: 
* Fraud detection (like for activity 3), we want to get the data real-time
* everything security-related, e.g. banking system
* stock pricing


# Activity 2
You run a temperature logging system in a small office. Sensors report the temperature once per minute and write the sensor readings into a PostgreSQL table.

* Low volume (~1 row per minute)
* Single consumer (reporting script)
* No real-time streaming needed

## Part 1
*In a simple use case where sensor readings need to be processed every 10 minutes to calculate the average temperature over that time window, describe which software architecture would be most appropriate for fetching the data from PostgreSQL, and explain the rationale behind your choice.*

As the volume of the data is very small, there is only 1 consumer, and it is only necessary to fetch every 10 minutes, I would suggest to simply poll the PostgreSQL-DB every 10 min using a Python script. 

Using Kafka and/or Debezium CDC would be over-engineered for this use case. 

## Part 2
*From the architectural choice made in ```Part 1```, implement the solution to consume and processing the data generated by the ```temperature_data_producer.py``` file (revise its features!). The basic logic from the file ```temperature_data_consumer.py``` should be extended with the connection to data source defined in ```Part 1```'s architecture.*

```python
# Step 1: Connect to database
conn = psycopg2.connect(
    dbname=DB_NAME, 
    user=DB_USER,
    password=DB_PASSWORD,
    host=DB_HOST,
    port=DB_PORT
)
cursor = conn.cursor()
# Step 2: Get AVG temperature of last 10 minutes
cursor.execute(sql.SQL("SELECT AVG(temperature) FROM temperature_readings WHERE recorded_at >= %s"), [ten_minutes_ago])
avg_temp = cursor.fetchone()[0]
```

## Part 3
*Discuss the proposed architecture in terms of resource efficiency, operability, and deployment complexity. This includes analyzing how well the system utilizes compute, memory, and storage resources; how easily it can be operated, monitored, and debugged in production.*

As I did not use Kafka or Debezium CDC, only the PostgreSQL-DB and Python need to be deployed. 

My approach is resource-efficient as it has very low CPU usage and memory overhead. 

The Python Script can easily be extended to include more logging details. As the script is also very short and straightforward, and insights into PostgreSQL are also easily possible, debugging should also be fairly easy. 


# Activity 3
A robust fraud detection system operating at **high scale** must be designed to handle extremely **high data ingestion rates** while enabling near **real-time** analysis by **multiple independent consumers**. In this scenario, potentially hundreds of thousands of transactional records per second are continuously written into an OLTP PostgreSQL database (see an example simulating it with a data generator inside the folder ```Activity3```), which serves as the system of record and guarantees strong consistency, durability, and transactional integrity. Moreover, the records generated are needed by many consumers in near real-time (see inside the folder ```Activity3``` two examples simulating agents consuming the records and generating alerts). Alerts or enriched events generated by these agents can then be forwarded to downstream systems, such as alerting services, dashboards, or case management tools.

* High data volume (potentially hundreds of thousands of records per second)

* Multiple consumer agents

* Near real-time streaming needed

## Part 1
*Describe which software architecture would be most appropriate for fetching the data from PostgreSQL and generate alerts in real-time. Explain the rationale behind your choice.*

As for this use case it is important to stream the data in real-time, there is high data volume and multiple consumers, I would use Debezium CDC and Kafka to fetch the data from the database. 

This has the advantage that multiple systems can independently consume the data, without touching the DB at all. 

Direct polling as in the previous exercise will not scale at this ingestion rate and would not fulfill the requirement to fetch the data in real-time.

## Part 2
*From the architectural choice made in ```Part 1```, implement the 'consumer' to fetch and process the records generated by the ```fraud_data_producer.py``` file (revise its features!). The basic logic from the files ```fraud_consumer_agent1.py.py``` and ```fraud_consumer_agent2.py.py``` should be extended with the connection to data source defined in ```Part 1```'s architecture.*

For that to work I also executed the following command in the terminal to create a new Debezium CDC Connector: 

```bash
curl -i -X POST -H "Accept:application/json"   -H "Content-Type:application/json"   localhost:8083/connectors/   -d '{
    "name": "transactions-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "tasks.max": "1",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "postgrespw",
      "database.dbname": "mydb",
      "slot.name": "transactionsslot",
      "topic.prefix": "dbserver1",
      "plugin.name": "pgoutput",
      "database.replication.slot.name": "debeziumtransactions",

      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "false",
      "decimal.handling.mode": "double"
    }
  }'
```


## Part 3
*Discuss the proposed architecture in terms of resource efficiency, operability, maintainability, deployment complexity, and overall performance and scalability. This includes discussing how well the system utilizes compute, memory, and storage resources; how easily it can be operated, monitored, and debugged in production; how maintainable and evolvable the individual components are over time; the effort required to deploy and manage the infrastructure; and the systemâ€™s ability to sustain increasing data volumes, higher ingestion rates, and a growing number of fraud detection agents without degradation of latency or reliability.*

The PostgreSQL-DB is protected against read overload. Higher CPU- and memory-usage since there is also more data being processed. 

There are clear component boundaries (DB, Debezium CDC Connector, Kafka, Agents). 
It is also easy to replay and recover in case of failure with Kafka (for consumers with offset, when more than one broker). Kafka also provides central monitoring.  

Kafka scales horizontally by adding partitions and brokers, though increasing partitions may require consumer rebalancing.

Deployment is more complex compared to the architecture in Activity 2, but I can imagine that for such popular tools as Kafka and Debezium, there can be enough information on how to do that found. 

If one wants to add other fraud detection agents in the future that is easily done by creating another Python script. 


## Part 4
*Compare the proposed architecture to Exercise 3 from previous lecture where the data from PostgreSQL was loaded to Spark (as a consumer) using the JDBC connector. Discuss both approaches at least in terms of performance, resource efficiency, and deployment complexity.*

With Kafka and Debezium it is possible to fetch the data in real-time with minimal DB load. In the JDBC-based Spark approach, data ingestion is batch or micro-batch oriented and requires repeated database reads, which increases database load and latency compared to CDC-based streaming.

It is very easy to scale Kafka, it is harder to do that with Spark but still possible.

Deploying Spark is simpler than Kafka and Debezium as there is only 1 service to be deployed compared to 2 and Kafka consists of many components that need to be deployed (brokers, consumers, UI). 